#######################################################################################################
# This script is used to generate a list of metadata jsons for documents crawled and deposited by a
# given spider in Gamechanger's S3 bucket:
#   - s3://advana-data-zone/bronze/gamechanger/pdf/
#
# The script is intended to run locally against metadata files downloaded from the s3 bucket. Download
# In the terminal, navigate to the folder where the metadata documents are to be downloaded and run:
#   - aws s3 cp s3://advana-data-zone/bronze/gamechanger/pdf/ . --exclude="*" --include="*.metadata" \
#       --recursive &> /dev/null&
#
# The crawler who's documents are to be targeted must be defined in the main block of this script. A target directory
# path, as well as an output directory path, can also be defined, or the os.getcwd() default will assign
# the current directory. The resulting compiled json can then be used to delete the documents included
# from the full Gamechanger corpus.
#######################################################################################################

import os
import json
from pathlib import Path

from configuration.utils import get_connection_helper_from_env

def query_uncat_es(crawler):
    body = {
        "query": {
            "bool": {
                "must" : [
                {
                    "term": {
                        "crawler_used_s": "air_force_pubs"
                    }
                },
                {
                    "term": {
                        "display_doc_type_s": "Uncategorized"
                    }
                },
                {
                    "term": {
                        "is_revoked_b": False
                    }
                }
            ]
        }
    },
    "size": 10000,
    "fields": ["id", "original_ingest_date", "current_ingest_date", "display_org_s", "filename"],
    "_source": False
    }

def get_current_gc_alias(ch):
    es_response = ch.es_client.cat.aliases()
    return list(filter(lambda x: (x != ''), [line for line in es_response.split('\n') if line.startswith('gamechanger ')][0].split(' ')))[1]

def get_target_jsons(crawler, dir):
    """generate json file containing jsons compiled from metadata generated by a given crawler"""
    files = [file for file in dir.glob("*.metadata")]
    lines = []
    for filename in files:
        with open(filename, 'r') as f:
            data = json.load(f)
            if data['crawler_used'] == crawler:
                data['filename'] = Path(filename).stem
                lines.append(data)
    return lines

def write_output_json(lines, output_file, result_dir):
    with open(os.path.join(result_dir, output_file), mode='a') as new_file:
        cnt = 0
        for line in lines:
            jsondoc = json.dumps(line) + '\n'
            new_file.write(jsondoc)
            cnt += 1
    return "Metadata from " + str(cnt) + " documents compiled in " + output_file


def main():
    crawler = "dha_pubs" # Crawler to get metadata jsons from
    dir = Path(os.getcwd(), "metadata") # Directory path of metadata jsons for the full GC corpus
    output_file = "test_output.json" # Filename for the compiled json resulting from this script
    result_dir = os.getcwd() # Directory path where the output json is to be written
    lines = get_target_jsons(crawler, dir)
    result_txt = write_output_json(lines, output_file, result_dir)
    print(result_txt)

if __name__ == "__main__":
    main()