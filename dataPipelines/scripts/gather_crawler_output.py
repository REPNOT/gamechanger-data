#######################################################################################################
# This script is used to generate a list of metadata jsons for documents crawled and deposited by a
# given spider in Gamechanger's S3 bucket:
#   - s3://advana-data-zone/bronze/gamechanger/pdf/
#
# The script is intended to run locally against metadata files downloaded from the s3 bucket. Download
# In the terminal, navigate to the folder where the metadata documents are to be downloaded and run:
#   - aws s3 cp s3://advana-data-zone/bronze/gamechanger/pdf/ . --exclude="*" --include="*.metadata" \
#       --recursive &> /dev/null&
#
# The crawler who's documents are to be targeted must be defined in the main block of this script. A target directory
# path, as well as an output directory path, can also be defined, or the os.getcwd() default will assign
# the current directory. The resulting compiled json can then be used to delete the documents included
# from the full Gamechanger corpus.
#######################################################################################################

import os
import json
from pathlib import Path

def get_target_jsons(crawler, dir):
    """generate json file containing jsons compiled from metadata generated by a given crawler"""
    files = [file for file in dir.glob("*.metadata")]
    lines = []
    for filename in files:
        with open(filename, 'r') as f:
            data = json.load(f)
            if data['crawler_used'] == crawler:
                data['filename'] = Path(filename).stem
                lines.append(data)
    return lines

def write_output_json(lines, output_file, result_dir):
    with open(os.path.join(result_dir, output_file), mode='a') as new_file:
        cnt = 0
        for line in lines:
            jsondoc = json.dumps(line) + '\n'
            new_file.write(jsondoc)
            cnt += 1
    return "Metadata from " + str(cnt) + " documents compiled in " + output_file


if __name__ == "__main__":
    crawler = "dha_pubs" # Crawler to get metadata jsons from
    dir = Path(os.getcwd(), "metadata") # Directory path of metadata jsons for the full GC corpus
    output_file = "test_output.json" # Filename for the compiled json resulting from this script
    result_dir = os.getcwd() # Directory path where the output json is to be written
    lines = get_target_jsons(crawler, dir)
    result_txt = write_output_json(lines, output_file, result_dir)
    print(result_txt)
